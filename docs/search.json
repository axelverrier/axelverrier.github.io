[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My current and past projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nJun 3, 2025\n\n\nCode snippets\n\n\nCode snippets in R, Python and Stata. Updated once in a while.\n\n\n\n\nMar 14, 2024\n\n\nOECD workshop on R\n\n\nR4DEV workshop from my internship at OECD\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "projects/snippets/wrangling.html",
    "href": "projects/snippets/wrangling.html",
    "title": "Data wrangling",
    "section": "",
    "text": "Loading data\n\nRPythonStata\n\n\nfile = \"\"\n\n#csv\ndata &lt;- read.csv(file, header=TRUE, sep=\";\")\ndata &lt;- haven::read_dta(file)\n\n#RData Rda\ndata &lt;- load(file)\n\n#RDS\ndata &lt;- readRDS(file) \n\n#Stata files (.dta)\nlibrary(heaven)\nheaven::read_dta(file)\n\n\nimport pandas as pd\n\nfile = \"\"\n\n#csv\ndata = pd.read_csv(file, sep=\",\", \n                    header=\"infer\", \n                    names=None, \n                    usecols=None, \n                    dtype=None, \n                    converters=None, \n                    skiprows=None, \n                    skipfooter=0, \n                    nrows=None)\n\n#Stata files (.dta)\ndata = pd.read_stata(file, columns=None)\n\n\n#csv\nimport delimited \"filename.csv\", clear varnames(1) encoding(utf8) case(lower) delimiter(\";\") rowrange() colrange()\nuse \"filename.dta\", clear\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/snippets/statistics.html",
    "href": "projects/snippets/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "RPythonStata\n\n\nlibrary(tidyverse)\nlibrary(e1071)\nlibrary(stargazer)\n\nbasic_stats &lt;- function(x) {\n  c(\n    mean = mean(x, na.rm = TRUE),\n    sd = sd(x, na.rm = TRUE),\n    skewness = e1071::skewness(x, na.rm = TRUE),\n    kurtosis = e1071::kurtosis(x, na.rm = TRUE),\n    Q10 = quantile(x, 0.10, na.rm = TRUE, names = FALSE),\n    Q25 = quantile(x, 0.25, na.rm = TRUE, names = FALSE),\n    median = median(x, na.rm = TRUE),\n    Q75 = quantile(x, 0.75, na.rm = TRUE, names = FALSE),\n    Q90 = quantile(x, 0.90, na.rm = TRUE, names = FALSE),\n    IQR = IQR(x, na.rm = TRUE), \n    NAs = sum(is.na(x))\n  )\n}\n\ndf_summary_stats &lt;- data %&gt;% \n  select(where(is.numeric)) %&gt;%  # Select only numeric columns\n  map_dfr(basic_stats, .id = \"variable\") %&gt;%\n  mutate(across(everything(), ~round(.x, 2)))\n\n#latex\nstargazer(df_summary_stats, summary = FALSE, type = \"latex\", no.space = TRUE, column.sep.width = \"1pt\")\n#csv\nwrite.csv(df_summary_stats, \"summary_stats.csv\", row.names = FALSE) #comma delimiter\nwrite.table(df_summary_stats, \"summary_stats.csv\", sep = \";\", row.names = FALSE, col.names = TRUE) #other delimiters\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import skew, kurtosis\nfrom tabulate import tabulate\n\n# Define the basic_stats function\ndef basic_stats(x):\n    return {\n        'mean': x.mean(),\n        'sd': x.std(),\n        'skewness': skew(x, nan_policy='omit'),\n        'kurtosis': kurtosis(x, nan_policy='omit'),\n        'Q10': x.quantile(0.10),\n        'Q25': x.quantile(0.25),\n        'median': x.median(),\n        'Q75': x.quantile(0.75),\n        'Q90': x.quantile(0.90),\n        'IQR': x.quantile(0.75) - x.quantile(0.25),\n        'NAs': x.isna().sum()\n    }\n\n#not working properly yet\n# df_summary_stats = data.select_dtypes(include=[np.number]).apply(basic_stats).T\n# df_summary_stats = df_summary_stats.round(2)\n# \n# #latex\n# latex_table = tabulate(df_summary_stats, headers='keys', tablefmt='latex', showindex=True)\n# with open(\"summary_stats.tex\", \"w\") as f:\n#     f.write(latex_table)\n# \n# #csv\n# df_summary_stats.to_csv(\"summary_stats_semicolon.csv\", sep=';', index=True)\n\n\n\nlocal numeric_vars\nforeach var of varlist `numeric_vars' {\n    robstat `var', statistics(mean sd skewness kurtosis median IQR)\n    estimates store stats_`var'\n}\n\nesttab stats_* using stats.tex, label nostar compress nogaps\nesttab stats_* using stats.csv, label nostar compress nogaps\n\n\n\n\n\n\n\nRStata\n\n\nlibrary(tidyverse)\nlibrary(e1071)\nlibrary(stargazer)\nlibrary(DescTools)\nlibrary(robustbase)\n\n\n\nstat_location &lt;- function(x) {\n  c(\n    mean = mean(x, na.rm = TRUE),\n    trimmed_mean = mean(x, trim = 0.05, na.rm = TRUE),\n    median = median(x, na.rm = TRUE),\n    HL = DescTools::HodgesLehmann(x, na.rm=TRUE)\n    )\n}\nstat_scale &lt;- function(x) {\n  c(\n    sd = sd(x, na.rm = TRUE),\n    IQR = IQR(x, na.rm = TRUE),\n    MAD = mad(x, na.rm = TRUE),\n    robustbase::Qn(x, na.rm = TRUE)\n    )\n}\n\ndf &lt;- data %&gt;% \n  select(where(is.numeric)) %&gt;%  # Select only numeric columns\n  map_dfr(stat_location, .id = \"variable\") %&gt;%\n  mutate(across(everything(), ~round(.x, 2)))\n\ndf &lt;- data %&gt;% \n  select(where(is.numeric)) %&gt;%  # Select only numeric columns\n  map_dfr(stat_scale, .id = \"variable\") %&gt;%\n  mutate(across(everything(), ~round(.x, 2)))\n\n\n#export latex\nstargazer(df, summary = FALSE, type = \"latex\", no.space = TRUE, column.sep.width = \"1pt\")\n\n#export csv\nwrite.csv(df, \"summary_stats.csv\", row.names = FALSE) #comma delimiter\nwrite.table(df, \"summary_stats.csv\", sep = \";\", row.names = FALSE, col.names = TRUE) #other delimiters\n\n\nlocal numeric_vars\nforeach var of varlist `numeric_vars' {\n    robstat `var', statistics(mean alpha5 median HL)\n    estimates store rb_loc_`var'\n}\n\nforeach var of varlist `numeric_vars' {\n    robstat `var', statistics(sd IQR MAD Qn)\n    estimates store rb_scale_`var'\n}\n\n\n//export tex \nesttab rb_loc_* using rb_stats_loc.tex, label nostar compress nogaps\nesttab rb_scale_* using rb_stats_scale.tex, label nostar compress nogaps\n\n//export csv \nesttab rb_loc_* using rb_stats_loc.csv, label nostar compress nogaps\nesttab rb_scale_* using rb_stats_scale.csv, label nostar compress nogaps\n\n\n\n\n\n\n\nRPython\n\n\nlibrary(tidyverse)\n\nstats_qual &lt;- function(x) {\n  mode_val &lt;- names(sort(table(x), decreasing = TRUE))[1]\n  mode_prop &lt;- round(sort(table(x), decreasing = TRUE)[[1]] / sum(table(x)), 1)\n  \n  data.frame(\n    unique = length(unique(x)), \n    NAs = sum(is.na(x)), \n    mode = paste0(mode_val, \" (\", mode_prop, \")\")\n  )\n}\n\ndf_summary_stats &lt;- data %&gt;%\n  select(where(is.factor)) %&gt;%\n  map_dfr(stats_qual, .id = \"variable\")\n\n#csv\nwrite.csv(df_summary_stats, \"summary_stats.csv\", row.names = FALSE) #comma delimiter\nwrite.table(df_summary_stats, \"summary_stats.csv\", sep = \";\", row.names = FALSE, col.names = TRUE) #other delimiters"
  },
  {
    "objectID": "projects/snippets/statistics.html#summary-statistics-basic",
    "href": "projects/snippets/statistics.html#summary-statistics-basic",
    "title": "Statistics",
    "section": "",
    "text": "RPythonStata\n\n\nlibrary(tidyverse)\nlibrary(e1071)\nlibrary(stargazer)\n\nbasic_stats &lt;- function(x) {\n  c(\n    mean = mean(x, na.rm = TRUE),\n    sd = sd(x, na.rm = TRUE),\n    skewness = e1071::skewness(x, na.rm = TRUE),\n    kurtosis = e1071::kurtosis(x, na.rm = TRUE),\n    Q10 = quantile(x, 0.10, na.rm = TRUE, names = FALSE),\n    Q25 = quantile(x, 0.25, na.rm = TRUE, names = FALSE),\n    median = median(x, na.rm = TRUE),\n    Q75 = quantile(x, 0.75, na.rm = TRUE, names = FALSE),\n    Q90 = quantile(x, 0.90, na.rm = TRUE, names = FALSE),\n    IQR = IQR(x, na.rm = TRUE), \n    NAs = sum(is.na(x))\n  )\n}\n\ndf_summary_stats &lt;- data %&gt;% \n  select(where(is.numeric)) %&gt;%  # Select only numeric columns\n  map_dfr(basic_stats, .id = \"variable\") %&gt;%\n  mutate(across(everything(), ~round(.x, 2)))\n\n#latex\nstargazer(df_summary_stats, summary = FALSE, type = \"latex\", no.space = TRUE, column.sep.width = \"1pt\")\n#csv\nwrite.csv(df_summary_stats, \"summary_stats.csv\", row.names = FALSE) #comma delimiter\nwrite.table(df_summary_stats, \"summary_stats.csv\", sep = \";\", row.names = FALSE, col.names = TRUE) #other delimiters\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import skew, kurtosis\nfrom tabulate import tabulate\n\n# Define the basic_stats function\ndef basic_stats(x):\n    return {\n        'mean': x.mean(),\n        'sd': x.std(),\n        'skewness': skew(x, nan_policy='omit'),\n        'kurtosis': kurtosis(x, nan_policy='omit'),\n        'Q10': x.quantile(0.10),\n        'Q25': x.quantile(0.25),\n        'median': x.median(),\n        'Q75': x.quantile(0.75),\n        'Q90': x.quantile(0.90),\n        'IQR': x.quantile(0.75) - x.quantile(0.25),\n        'NAs': x.isna().sum()\n    }\n\n#not working properly yet\n# df_summary_stats = data.select_dtypes(include=[np.number]).apply(basic_stats).T\n# df_summary_stats = df_summary_stats.round(2)\n# \n# #latex\n# latex_table = tabulate(df_summary_stats, headers='keys', tablefmt='latex', showindex=True)\n# with open(\"summary_stats.tex\", \"w\") as f:\n#     f.write(latex_table)\n# \n# #csv\n# df_summary_stats.to_csv(\"summary_stats_semicolon.csv\", sep=';', index=True)\n\n\n\nlocal numeric_vars\nforeach var of varlist `numeric_vars' {\n    robstat `var', statistics(mean sd skewness kurtosis median IQR)\n    estimates store stats_`var'\n}\n\nesttab stats_* using stats.tex, label nostar compress nogaps\nesttab stats_* using stats.csv, label nostar compress nogaps"
  },
  {
    "objectID": "projects/snippets/statistics.html#summary-statistics-robust",
    "href": "projects/snippets/statistics.html#summary-statistics-robust",
    "title": "Statistics",
    "section": "",
    "text": "RStata\n\n\nlibrary(tidyverse)\nlibrary(e1071)\nlibrary(stargazer)\nlibrary(DescTools)\nlibrary(robustbase)\n\n\n\nstat_location &lt;- function(x) {\n  c(\n    mean = mean(x, na.rm = TRUE),\n    trimmed_mean = mean(x, trim = 0.05, na.rm = TRUE),\n    median = median(x, na.rm = TRUE),\n    HL = DescTools::HodgesLehmann(x, na.rm=TRUE)\n    )\n}\nstat_scale &lt;- function(x) {\n  c(\n    sd = sd(x, na.rm = TRUE),\n    IQR = IQR(x, na.rm = TRUE),\n    MAD = mad(x, na.rm = TRUE),\n    robustbase::Qn(x, na.rm = TRUE)\n    )\n}\n\ndf &lt;- data %&gt;% \n  select(where(is.numeric)) %&gt;%  # Select only numeric columns\n  map_dfr(stat_location, .id = \"variable\") %&gt;%\n  mutate(across(everything(), ~round(.x, 2)))\n\ndf &lt;- data %&gt;% \n  select(where(is.numeric)) %&gt;%  # Select only numeric columns\n  map_dfr(stat_scale, .id = \"variable\") %&gt;%\n  mutate(across(everything(), ~round(.x, 2)))\n\n\n#export latex\nstargazer(df, summary = FALSE, type = \"latex\", no.space = TRUE, column.sep.width = \"1pt\")\n\n#export csv\nwrite.csv(df, \"summary_stats.csv\", row.names = FALSE) #comma delimiter\nwrite.table(df, \"summary_stats.csv\", sep = \";\", row.names = FALSE, col.names = TRUE) #other delimiters\n\n\nlocal numeric_vars\nforeach var of varlist `numeric_vars' {\n    robstat `var', statistics(mean alpha5 median HL)\n    estimates store rb_loc_`var'\n}\n\nforeach var of varlist `numeric_vars' {\n    robstat `var', statistics(sd IQR MAD Qn)\n    estimates store rb_scale_`var'\n}\n\n\n//export tex \nesttab rb_loc_* using rb_stats_loc.tex, label nostar compress nogaps\nesttab rb_scale_* using rb_stats_scale.tex, label nostar compress nogaps\n\n//export csv \nesttab rb_loc_* using rb_stats_loc.csv, label nostar compress nogaps\nesttab rb_scale_* using rb_stats_scale.csv, label nostar compress nogaps"
  },
  {
    "objectID": "projects/snippets/statistics.html#summary-statistics-for-qualitative-variables",
    "href": "projects/snippets/statistics.html#summary-statistics-for-qualitative-variables",
    "title": "Statistics",
    "section": "",
    "text": "RPython\n\n\nlibrary(tidyverse)\n\nstats_qual &lt;- function(x) {\n  mode_val &lt;- names(sort(table(x), decreasing = TRUE))[1]\n  mode_prop &lt;- round(sort(table(x), decreasing = TRUE)[[1]] / sum(table(x)), 1)\n  \n  data.frame(\n    unique = length(unique(x)), \n    NAs = sum(is.na(x)), \n    mode = paste0(mode_val, \" (\", mode_prop, \")\")\n  )\n}\n\ndf_summary_stats &lt;- data %&gt;%\n  select(where(is.factor)) %&gt;%\n  map_dfr(stats_qual, .id = \"variable\")\n\n#csv\nwrite.csv(df_summary_stats, \"summary_stats.csv\", row.names = FALSE) #comma delimiter\nwrite.table(df_summary_stats, \"summary_stats.csv\", sep = \";\", row.names = FALSE, col.names = TRUE) #other delimiters"
  },
  {
    "objectID": "projects/2024-oecd-r-workshop.html",
    "href": "projects/2024-oecd-r-workshop.html",
    "title": "OECD workshop on R",
    "section": "",
    "text": "This is the results of an OECD workshop on R called R4DEV created and taught by Nelson Amaya. It took place from March to June 2024. During this workshop, I learned how to use R for text analysis, data visualisation: (plots, animated graphs and maps), web scrapping, shiny apps and quarto publishing.\nFind my achievements below.\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nMay 4, 2024\n\n\nWeb scrapping\n\n\nUsing web scrapping techniques to gather data from the web\n\n\n\n\nApr 9, 2024\n\n\nAnimating graphs\n\n\nProducing animated graphs\n\n\n\n\nMar 31, 2024\n\n\nText and sentiment analysis\n\n\nPerforming sentiment analysis on books.\n\n\n\n\nMar 14, 2024\n\n\nPlotting data\n\n\nVisualisation using ggplot and interactive graphs.\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "projects/2024-oecd-r-workshop/04_animation.html",
    "href": "projects/2024-oecd-r-workshop/04_animation.html",
    "title": "Animating graphs",
    "section": "",
    "text": "In this post, I reproduce a graph but making an animation out of it using the package gganimate. The graph I use is from Gilbert Fontana whose code is available here.\nI made some minor adjustment for it to work with gganimate, and added the animation part.\nFirst, let’s load the packages\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(openxlsx)\nlibrary(gganimate)\n\n\nThen I load the data.\n\n\nShow the code\ndata = openxlsx::read.xlsx(\"https://github.com/holtzy/R-graph-gallery/raw/master/DATA/wealth_data.xlsx\",sheet=1)\n\n\nSome minor preparation details.\n\n\nShow the code\n#color palette\npal=c(\"#003f5c\",\n      \"#2f4b7c\",\n      \"#665191\",\n      \"#a05195\",\n      \"#d45087\",\n      \"#f95d6a\",\n      \"#ff7c43\",\n      \"#ffa600\")\n\n# Stacking order\norder &lt;- c(\"United States\", \"China\", \"Japan\", \"Germany\", \"United Kingdom\", \"France\", \"India\", \"Other\")\n\ntheme_set(theme_minimal(base_size = 3))\n\n\nAnd finally, the plot, in which I incorporated a gganimate element : transition_reveal()\n\n\nShow the code\nplot &lt;- data %&gt;%\n  mutate(country = factor(country, levels=order)) %&gt;% \n  ggplot(aes(year, total_wealth, fill = country, label = country, color = country)) +\n  geom_area() +\n  gganimate::transition_reveal(after_stat(x))+\n  view_follow(fixed_x = TRUE) +\n  \n  #Title\n  annotate(\"text\", x = 2000, y = 100000,\n           label = \"Aggregated\\nHousehold\\nWealth\",\n           hjust=0,\n           vjust=-1.9,\n           size=12,\n           lineheight=.9,\n           fontface=\"bold\",\n           color=\"black\") +\n\n  #USA\n  annotate(\"text\", x = 2021.2, y = 370000,\n           label = \"USA $145,793B\",\n           hjust=0,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=pal[1]) +\n  #China\n  annotate(\"text\", x = 2021.2, y = 270000,\n           label = \"China $85,107B\",\n           hjust=0,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=pal[2]) +\n  #Japan\n  annotate(\"text\", x = 2021.2, y = 225000,\n           label = \"Japan $25,692B\",\n           hjust=0,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=pal[3]) +\n  #Germany\n  annotate(\"text\", x = 2021.2, y = 200000,\n           label = \"Germany $17,489B\",\n           hjust=0,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=pal[4]) +\n  #UK\n  annotate(\"text\", x = 2021.2, y = 180000,\n           label = \"UK $16,261B\",\n           hjust=0,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=pal[5]) +\n  #France\n  annotate(\"text\", x = 2021.2, y = 166000,\n           label = \"France $16,159B\",\n           hjust=0,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=pal[6]) +\n  #India\n  annotate(\"text\", x = 2021.2, y = 150000,\n           label = \"India $14,225B\",\n           hjust=0,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=pal[7]) +\n  #Other\n  annotate(\"text\", x = 2021.2, y = 80000,\n           label = \"Rest of the world\\n$142,841B\",\n           hjust=0,\n           size=3,\n           lineheight=1.5,\n           fontface=\"bold\",\n           color=pal[8]) +\n  \n  ## Vertical segments\n  geom_segment(aes(x = 2000, y = 0, xend = 2000, yend = 117426+20000),color=\"black\") +\n  geom_point(aes(x = 2000, y = 117426+20000),color=\"black\") +\n  annotate(\"text\", x = 2000, y = 117426+33000,\n           label = \"$117,844B\",\n           hjust=0.5,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=\"black\") +\n  \n  geom_segment(aes(x = 2005, y = 0, xend = 2005, yend = 181731+20000),color=\"black\") +\n  geom_point(aes(x = 2005, y = 181731+20000),color=\"black\") +\n  annotate(\"text\", x = 2005, y = 181731+33000,\n           label = \"$182,350B\",\n           hjust=0.5,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=\"black\") +\n  \n  geom_segment(aes(x = 2010, y = 0, xend = 2010, yend = 250932+20000),color=\"black\") +\n  geom_point(aes(x = 2010, y = 250932+20000),color=\"black\") +\n  annotate(\"text\", x = 2010, y = 250932+33000,\n           label = \"$251,885B\",\n           hjust=0.5,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=\"black\") +\n  \n  geom_segment(aes(x = 2015, y = 0, xend = 2015, yend = 296203+25000),color=\"black\") +\n  geom_point(aes(x = 2015, y = 296203+25000),color=\"black\") +\n  annotate(\"text\", x = 2015, y = 296203+38000,\n           label = \"$297,698B\",\n           hjust=0.5,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=\"black\") +\n  \n  geom_segment(aes(x = 2021, y = 0, xend = 2021, yend = 461370+50000),color=\"black\") +\n  geom_point(aes(x = 2021, y = 461370+50000),color=\"black\") +\n  annotate(\"text\", x = 2021, y = 461370+50000,\n           label = \"$463,567B\",\n           hjust=1.1,\n           size=3,\n           lineheight=.8,\n           fontface=\"bold\",\n           color=\"black\") +\n  \n  #Color scale\n  scale_fill_manual(values=pal) +\n  scale_color_manual(values=pal) +\n  scale_x_continuous(breaks=c(2000,2005,2010,2015,2021),labels = c(\"2000\",\"2005\",\"2010\",\"2015\",\"2021\")) +\n  scale_y_continuous(expand = c(0,0)) +\n  \n  #Last customization\n  coord_cartesian(clip = \"off\") +\n  xlab(\"\") +\n  ylab(\"\") +\n  labs(caption = \"Data: James Davies, Rodrigo Lluberas and Anthony Shorrocks, Credit Suisse Global Wealth Databook 2022\n       Design: Gilbert Fontana\n       Animation: Axel Verrier\") +\n  theme(\n    axis.line.x = element_line(linewidth = .75),\n    panel.grid = element_blank(),\n    axis.text.y=element_blank(),\n    axis.text.x = element_text(color=\"black\", size=10,margin = margin(5,0,0,0)),\n    plot.margin = margin(20,120,20,20),\n    legend.position = \"none\",\n    plot.caption =element_text(hjust=0, margin=margin(50,0,0,0), size = 8, lineheight = 1.5)\n  )\n\n\nFinally, the plot is transformed into a gif and saved.\n\n\nShow the code\nanimate(plot,\n        fps = 10,\n        duration = 20,\n        width=800, \n        end_pause = 80, \n        renderer = gifski_renderer()) \n\n\nanim_save(paste0(getwd(), \"/projects/2024-oecd-r-workshop/wealth.gif\"),\n          animation = last_animation())\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/2024-oecd-r-workshop/02_plots.html",
    "href": "projects/2024-oecd-r-workshop/02_plots.html",
    "title": "Plotting data",
    "section": "",
    "text": "Spotify song analysis\nLet’s first load the packages necessary for the analysis.\n\n\nShow the code\n# Charging dependencies\nlibrary(tidyverse) \nlibrary(spotifyr) \nlibrary(ggridges) \nlibrary(RColorBrewer) \nlibrary(MetBrewer) \nlibrary(plotly) \nlibrary(htmltools) \nlibrary(ggiraph)\n\n\nI define my token for accessing the Spotify API (hidden) and create the function for retrieving the artist ID.\n\n\nShow the code\nspotify_id &lt;- function(artist_name) {\n  spotifyr::search_spotify(print(artist_name), type = \"artist\") |&gt;\n    #dplyr::arrange(desc(popularity)) |&gt; #misleading the algorithm\n    dplyr::select(id) |&gt;\n    dplyr::slice(1) |&gt;\n    as.character()\n}\n\n\nThen I define some of my favorite artists in a tibble.\n\n\nShow the code\nfavorites &lt;- tribble(\n  ~artist,\n  \"Damso\",\n  \"Dooz Kawa\",\n  \"Connor Price\",\n  \"Twenty One Pilots\",\n) |&gt;\n  dplyr::rowwise() |&gt;\n  dplyr::mutate(artist_id = spotify_id(artist))\n\n#save in list\nfavorites_music &lt;- list() \n\n\nNow I retrieve the data using the Spotify API.\n\n\nShow the code\nfor(i in favorites$artist_id) {\n  \n  favorites_music[[i]] &lt;- spotifyr::get_artist_audio_features(artist = print(i),\n                                                              include_groups = \"album\",\n                                                              authorization =  spotify_access_token)\n  \n}\n\n\nFinally here is a graph of the 4 artists.\n\n\nShow the code\nspotify_covers_gg &lt;- favorites_music %&gt;%\n  purrr::map_df(bind_rows) %&gt;%\n  dplyr::left_join(favorites, by=\"artist_id\") |&gt; \n  dplyr::mutate(cover = map(album_images, ~.x[[2]][[2]])) |&gt;\n  ggplot(aes(y=energy,x=valence,color=artist_name))+\n  geom_point_interactive(\n    aes(shape = artist_name,\n        tooltip =  paste0(\n          \"&lt;div style='display: flex; align-items: center;'&gt;\",\n          \"&lt;img src='\", cover, \"' style='width:100px; height:auto; margin-right: 10px;'&gt;\",\n          \"&lt;div&gt;\",\n          \"&lt;b&gt;Artist:&lt;/b&gt; \", artist_name, \"&lt;br&gt;\",\n          \"&lt;b&gt;Song:&lt;/b&gt; \", track_name, \"&lt;br&gt;\",\n          \"&lt;b&gt;Album:&lt;/b&gt; \", album_name,\n          \"&lt;/div&gt;\",\n          \"&lt;/div&gt;\"\n        )),  \n    position = \"jitter\")+\n  geom_hline(yintercept = 0.5)+\n  geom_vline(xintercept = 0.5)+\n  annotate(\"text\", x = 0.2, y = 1, label = \"Turbulent/Angry\")+\n  annotate(\"text\", x = 0.8, y = 1, label = \"Happy/Joyful\")+\n  annotate(\"text\", x = 0.2, y = 0.1, label = \"Sad/Depressing\")+\n  annotate(\"text\", x = 0.8, y = 0.1, label = \"Chill/Peaceful\")+\n  labs(title = \"Energy vs. Valence by artist\", \n       caption = \"Source: Spotify data\")+\n  MetBrewer::scale_color_met_d(name = \"Ingres\")+\n  scale_shape_manual(values = c(1:8))+\n  theme_classic()+\n  theme(legend.position = \"top\",\n        legend.title = element_blank())\n\ngirafe(ggobj = spotify_covers_gg)\n\n\n\n\n\n\nMost of the music produced by the four artists I chose is high-energy music. Dooz Kawa musics are rather happy and joyful (not so sure about that…) whereas Twenty One Pilots and Damso musics are more turbulent. Connor Price’s songs are equally distributed between angry and joyful, which seems correct. Also, apparently, I don’t listen to low energy music unless it’s depressing…\n\n\nIndicator from OWID\nNow let’s build a nice graph from OWID data. Let’sfocus on CO2 emissions.\n\n\nShow the code\n#Loading dependencies\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(viridis)\nlibrary(hrbrthemes)\n\n\nI first download the dataset directly from Github.\n\n\nShow the code\nco2data &lt;- readr::read_csv(\"https://raw.githubusercontent.com/owid/co2-data/master/owid-co2-data.csv\") |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::mutate(continent = countrycode::countrycode(sourcevar = country, origin = \"country.name\", destination = \"continent\"))\n\n\nI then build a nice bubble chart, made interactive with plotly.\n\n\nShow the code\np &lt;- co2data |&gt;\n  filter(year==\"2018\") |&gt; \n  dplyr::select(country, population, gdp, co2_per_capita, co2, continent) |&gt;\n  dplyr::filter(!is.na(continent), !is.na(gdp)) |&gt;\n  mutate(gdppc=round(gdp/population, 0)) |&gt;\n  arrange(desc(co2_per_capita)) |&gt;\n  mutate(country = factor(country, country)) |&gt;\n  mutate(text = paste(\"Country: \", country, \"\\nCO2 per capita (t): \", co2_per_capita, \"\\nPopulation (M): \", round(population/1000000,2), \"\\nGdp per capita ($) : \", gdppc, sep=\"\")) |&gt;  \n  ggplot(aes(x=gdppc, y=co2_per_capita, size = population, color = continent, text=text)) +\n  geom_point(alpha=0.7) +\n  scale_size(range = c(1, 20), name=\"CO2 per capita (t)\") +\n  scale_x_log10(breaks = c(1000, 10000, 100000), \n                labels = c(\"1 000\", \"10 000\", \"100 000\"))+\n  scale_color_viridis(discrete=TRUE, guide=FALSE) +\n  theme_ipsum() +\n  theme(legend.position=\"none\")+\n  labs(title = \"Chart of country population size by CO2 emissions and GDP\", \n       x= \"Annual GDP ($/per capita, in log)\", \n       y= \"CO2 emissions (tons of CO2/per capita\")\npp &lt;- ggplotly(p, tooltip=\"text\") |&gt; \n  layout(annotations = \n           list(x = 1, y = -0.2, \n                text = \"Source: OWID Data\", \n                showarrow = F, \n                xref='paper', \n                yref='paper')\n  )\npp\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI must say this graph is pretty useless and it doesn’t show much information, but the keyword to remember here is “pretty”.\n\n\n\n\nUse of ggiraph\nLet’s create two side-by-side visualisations using ggiraph. We use the dataset epa2021 from the package openintro\n\n\nShow the code\n# Library\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(hrbrthemes)\nlibrary(viridis)\nlibrary(patchwork)\nlibrary(openintro)\n\n\nFirst we clean and arrange the data.\n\n\nShow the code\ndata(epa2021)\ndata &lt;- epa2021 |&gt; \n  mutate(luxury=mfr_name %in% c(\"Rolls-Royce\", \"Porsche\", \"Maserati\", \"aston martin\", \"Ferrari\", \"Lotus\", \"Jaguar Land Rover L\")) |&gt;\n  mutate(luxury=factor(luxury, levels = c(TRUE, FALSE), labels=c(\"High-end\", \"Not high-end\"))) |&gt;\n  group_by(mfr_name) |&gt;\n  mutate(avg_city_mpg = mean(city_mpg, na.rm=TRUE), \n         avg_hwy_mpg = mean(hwy_mpg, na.rm=TRUE)) |&gt;\n  ungroup() \n\n\nThen we create our first plot.\n\n\nShow the code\np1 &lt;- data |&gt; \n  ggplot() +\n  geom_segment_interactive(aes(x=forcats::fct_reorder2(mfr_name, desc(as.integer((luxury))), avg_city_mpg), xend=mfr_name, y=avg_city_mpg, yend=avg_hwy_mpg, tooltip=luxury, data_id=luxury), color=\"grey\", size=1.3)+\n  geom_point( aes(x=mfr_name, y=avg_city_mpg), color=rgb(0.2,0.7,0.1,0.5), size=3 ) +\n  geom_point( aes(x=mfr_name, y=avg_hwy_mpg), color=\"lightblue\", size=3 ) +\n  coord_flip()+\n  theme_ipsum() +\n  theme(\n    axis.line = element_line(colour = \"grey50\"),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_line(linetype = \"dashed\"),\n    #panel.background = element_rect(fill = \"#fbf9f4\", color = \"#fbf9f4\"),\n    plot.background = element_rect(fill = \"#fbf9f4\", color = \"#fbf9f4\"),\n    legend.position = \"bottom\"\n  ) +\n  labs(title = \"Average city/highway mileage\",\n       y= \"Average mileage\", x=NULL) +\n  scale_color_manual(values = c(\"lightblue\", rgb(0.2,0.7,0.1,0.5)),\n                     labels = c(\"City Mileage\", \"Highway Mileage\"))\n\n\nThen our second plot.\n\n\nShow the code\np2 &lt;- data |&gt;\n  ggplot( aes(x=forcats::fct_reorder(luxury, desc(as.integer((luxury)))), y=no_cylinders, fill=luxury)) +\n  geom_violin_interactive(aes(data_id=luxury, tooltip=luxury)) +\n  geom_jitter(color=\"black\", size=0.4, alpha=0.9) +\n  scale_fill_viridis(discrete = TRUE, alpha=0.6, option = \"G\") +\n  theme_ipsum() +\n  coord_flip()+\n  theme(\n    axis.line = element_line(colour = \"grey50\"),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_line(linetype = \"dashed\"),\n    #plot.background = element_rect(fill = \"#fbf9f4\", color = \"#fbf9f4\"),\n    legend.position = \"none\"\n  ) +\n  labs(title = \"Number of cylinders\",\n       y= \"Number of cylinders\", x=NULL)\n\n\nFinally we combine them using ggiraph\n\n\nShow the code\nggiraph::girafe(\n  ggobj = p1 + p2 +\n    plot_annotation(\n      title = 'Car\\'s characteristics by manufacturer and luxury type',\n      caption = 'Source: EPA (2021)', \n      theme = theme(plot.title = element_text(size = 25), \n      plot.background = element_rect(fill = \"#fbf9f4\", color = \"#fbf9f4\"))\n    ),\n  options = list(\n    opts_hover_inv(css = \"opacity:0.1;\")\n  ),\n  width_svg = 10,\n  height_svg = 6\n)\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI don’t understand why the geom_point legend set in the first graph by scale_colour_manual() doesn’t work…\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Papers",
    "section": "",
    "text": "Aklilu, A., D. Dussaux and A. Verrier (2025), “Quantifying digital innovation for the twin transition: Historical trends, current landscape and links with environmental innovation”, OECD Environment Working Papers, No. 261, OECD Publishing, Paris, https://doi.org/10.1787/8cc4cff0-en.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Axel Verrier",
    "section": "",
    "text": "I am a graduate students in economics from the Paris School of Economics (Paris, France), with a double degree in public policy evaluation. Here you can find more about my ongoing projects.\n\n\nOECD | Intern | March - August 2024\nEnvironment Division. Research on interlinkages between innovation and environment (critical raw materials, twin transition). Co-authored the resulting paper\nUniversity of Bordeaux | Research Assistant | September 2023 - January 2024\n\nFrench Ministry of Higher Education and Research | Statistician intern | April - August 2023\n\nINSERM/ISPED | Research assistant | May - June 2022\n\n\n\n\n Paris School of Economics, France ∙ 2024-2025\nMRes in Economics\nClasses: Environmental Economics (I and II), Industrial Organization (I and II), Empirical IO, Energy Economics and Climate Objectives, Market Design, Advanced Microeconometrics, Machine Learning\n University of Bordeaux, France ∙ 2021-2024\nMSc in Economics, specialization in analysis of public policies Magistère de Sciences Economiques, parcours MAGEVAL\n University of Florence, Italy ∙ 2022\nSemester abroad\n\n\n\n\n\n\n\nR\nPython\nStata\nSQL\n\n\n\n\nLaTeX\nQuarto"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Axel Verrier",
    "section": "",
    "text": "OECD | Intern | March - August 2024\nEnvironment Division. Research on interlinkages between innovation and environment (critical raw materials, twin transition). Co-authored the resulting paper\nUniversity of Bordeaux | Research Assistant | September 2023 - January 2024\n\nFrench Ministry of Higher Education and Research | Statistician intern | April - August 2023\n\nINSERM/ISPED | Research assistant | May - June 2022"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Axel Verrier",
    "section": "",
    "text": "Paris School of Economics, France ∙ 2024-2025\nMRes in Economics\nClasses: Environmental Economics (I and II), Industrial Organization (I and II), Empirical IO, Energy Economics and Climate Objectives, Market Design, Advanced Microeconometrics, Machine Learning\n University of Bordeaux, France ∙ 2021-2024\nMSc in Economics, specialization in analysis of public policies Magistère de Sciences Economiques, parcours MAGEVAL\n University of Florence, Italy ∙ 2022\nSemester abroad"
  },
  {
    "objectID": "index.html#cs-skills",
    "href": "index.html#cs-skills",
    "title": "Axel Verrier",
    "section": "",
    "text": "R\nPython\nStata\nSQL\n\n\n\n\nLaTeX\nQuarto"
  },
  {
    "objectID": "posts/posts.html",
    "href": "posts/posts.html",
    "title": "",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "projects/2024-oecd-r-workshop/03_text.html",
    "href": "projects/2024-oecd-r-workshop/03_text.html",
    "title": "Text and sentiment analysis",
    "section": "",
    "text": "Let’s first load the packages necessary for the analysis.\n\n\nShow the code\n# Charging dependencies\nlibrary(gutenbergr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(stopwords)\nlibrary(hunspell)\nlibrary(SnowballC)\nlibrary(ggpattern)\nlibrary(ggwordcloud)\nlibrary(RColorBrewer)\nlibrary(textdata)\n\n\n\n\nShow the code\nwizard_of_oz &lt;- gutenberg_download(54) |&gt; select(text)\n\noz_df &lt;- wizard_of_oz |&gt;\n  stringr::str_squish() |&gt;\n  tibble::as_tibble() |&gt;\n  tidytext::unnest_tokens(input = \"value\", \n                          output = \"word\", \n                          token = \"words\",\n                          to_lower=TRUE) |&gt;\n  dplyr::anti_join(stopwords::stopwords(language = \"en\") |&gt;\n                     as_tibble(),\n                   by=c(\"word\"=\"value\")) |&gt;\n  dplyr::mutate(stem = SnowballC::wordStem(word)) \n\n\n\n\nShow the code\nbar_graph &lt;- oz_df |&gt;\n  dplyr::group_by(word) |&gt;\n  dplyr::mutate(word_count = n()) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::arrange(desc(word_count)) |&gt;\n  dplyr::distinct(word, .keep_all = TRUE) |&gt;\n  dplyr::slice_max(order_by = word_count, n=30) |&gt;\n  ggplot(aes(x=word |&gt; reorder(word_count),y=word_count, fill=word_count))+ \n  ggpattern::geom_col_pattern(\n    aes(pattern_fill = word_count),\n    pattern = 'none',\n    fill=\"seagreen4\",\n    show.legend = FALSE)+\n  geom_label(aes(label=word_count), size = 3, color=\"white\",hjust=-0.5)+\n  coord_flip()+ \n  ylim(c(0,2500))+ \n  labs(x=NULL,y=NULL, \n       title=\"Words in The Jungle Book\",\n       subtitle = \"30 most frequent words.\")\n\nbar_graph\n\n\nWarning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\n\n\n\n\n\n\n\nLet’s turn it into a wordcloud.\n\n\nShow the code\nwordcloud &lt;- oz_df |&gt;\n  dplyr::group_by(word) |&gt; \n  dplyr::summarise(word_count = n()) |&gt; \n  dplyr::distinct(word, .keep_all = TRUE) |&gt; \n  dplyr::slice_max(order_by=word_count, n = 100) |&gt;\n  ggplot()+ \n  ggwordcloud::geom_text_wordcloud(aes(label = word, size = word_count, color=word_count)) +\n  scale_size_area(max_size = 17) +\n  scale_color_distiller(palette=\"RdYlGn\", direction=-1)+\n  theme_minimal()\n\nwordcloud"
  },
  {
    "objectID": "projects/2024-oecd-r-workshop/03_text.html#some-personnalisation",
    "href": "projects/2024-oecd-r-workshop/03_text.html#some-personnalisation",
    "title": "Text and sentiment analysis",
    "section": "Some personnalisation",
    "text": "Some personnalisation\nLet’s make our own theme so that the graph are easier to customize.\n\n\nShow the code\ntheme_axel &lt;- function () {\n  theme(\n    axis.line = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid.major.x = element_line(colour=\"black\", linetype = \"dashed\"),\n    panel.background = element_rect(fill = \"#fbf9f4\", color = \"#fbf9f4\"),\n    panel.border = element_rect(linetype = 1, fill=NA), \n    plot.background = element_rect(fill = \"#fbf9f4\", color = \"#fbf9f4\")\n  )\n}"
  },
  {
    "objectID": "projects/2024-oecd-r-workshop/03_text.html#bing",
    "href": "projects/2024-oecd-r-workshop/03_text.html#bing",
    "title": "Text and sentiment analysis",
    "section": "Bing",
    "text": "Bing\n\n\nShow the code\nplot_bing &lt;- sentiment_bing %&gt;%\n  ggplot(aes(x=sentiment,y=sentiment_count,fill=sentiment))+\n  geom_col(show.legend = FALSE)+\n  coord_flip()+\n  labs(x=NULL,y=\"Word count\",\n       title = \"Pride and Prejudice, Jane Austen\", \n       subtitle = \"Text analysis using Bing\")+\n  scale_fill_viridis(discrete = TRUE, option=\"turbo\") +\n  theme_axel()\n\nplot_bing"
  },
  {
    "objectID": "projects/2024-oecd-r-workshop/03_text.html#nrc",
    "href": "projects/2024-oecd-r-workshop/03_text.html#nrc",
    "title": "Text and sentiment analysis",
    "section": "NRC",
    "text": "NRC\n\n\nShow the code\nplot_nrc &lt;- sentiment_nrc %&gt;%\n  ggplot(aes(x=sentiment,y=sentiment_count,fill=sentiment))+\n  geom_col(show.legend = FALSE)+\n  coord_flip()+\n  labs(x=NULL,y=\"Word count\",\n       title = \"Pride and Prejudice, Jane Austen\", \n       subtitle = \"Text analysis using NRC\")+\n  scale_fill_viridis(discrete = TRUE, option=\"turbo\") +\n  theme_axel()\n\nplot_nrc"
  },
  {
    "objectID": "projects/2024-oecd-r-workshop/03_text.html#lougrhan",
    "href": "projects/2024-oecd-r-workshop/03_text.html#lougrhan",
    "title": "Text and sentiment analysis",
    "section": "LOUGRHAN",
    "text": "LOUGRHAN\n\n\nShow the code\nplot_loughran &lt;- sentiment_loughran %&gt;%\n  ggplot(aes(x=sentiment,y=sentiment_count,fill=sentiment))+\n  geom_col(show.legend = FALSE)+\n  coord_flip()+\n  labs(x=NULL,y=\"Word count\",\n       title = \"Pride and Prejudice, Jane Austen\", \n       subtitle = \"Text analysis using LOUGHRAN\")+\n  scale_fill_viridis(discrete = TRUE, option=\"turbo\") +\n  theme_axel()\n\nplot_loughran"
  },
  {
    "objectID": "projects/2024-oecd-r-workshop/03_text.html#afinn",
    "href": "projects/2024-oecd-r-workshop/03_text.html#afinn",
    "title": "Text and sentiment analysis",
    "section": "AFINN",
    "text": "AFINN\n\n\nShow the code\nplot_afinn &lt;- sentiment_afinn %&gt;%\n  ggplot(aes(x=value,y=n,color=c))+\n  geom_col(show.legend = FALSE)+\n  geom_vline(xintercept = 0)+\n  scale_x_continuous(breaks=seq(from=-5,to=5,by=1))+\n  annotate(\"text\",x=-3.5,y=4e+06, label =\"Negative\", color = \"#30123BFF\")+\n  annotate(\"text\",x=0.1,y=4e+06, label =\"Positive\", color = \"#7A0403FF\")+\n  labs(x=NULL,y=\"Word frequency\",\n       title = \"Pride and Prejudice, Jane Austen\", \n       subtitle = \"Text analysis using Afinn\")+\n  scale_color_viridis(discrete = TRUE, option=\"turbo\") +\n  theme_axel()\n\nplot_afinn"
  },
  {
    "objectID": "projects/2024-oecd-r-workshop/03_text.html#comparison",
    "href": "projects/2024-oecd-r-workshop/03_text.html#comparison",
    "title": "Text and sentiment analysis",
    "section": "Comparison",
    "text": "Comparison\n\nTable 1: Comparison of sentiment lexicon available in tidytext.\n\n\n\n\n\n\n\nSentiment lexicon\nPros\nCons\n\n\n\n\nBing\nEasy to understand\nLimited information\n\n\nNRC\nExtensive information (sentiments AND emotions)\nto be found ?\n\n\nLOUGHRAN\nNew insights and nuances on what is expressed in the text\nCreated for use with financial documents, maybe not the most appropriate lexicon for analyzing books\n\n\nAFINN\nAllows intensity analysis\nLimited information"
  },
  {
    "objectID": "projects/2024-oecd-r-workshop/06_scrapping.html",
    "href": "projects/2024-oecd-r-workshop/06_scrapping.html",
    "title": "Web scrapping",
    "section": "",
    "text": "Let’s start by installing the required dependencies.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(rlang)\nlibrary(reactable)\n\n\n\n1. Get a list of someone’s papers from Google scholar\nSince there is no API, we define a function to retrieve any author’s ID.\n\n\nShow the code\nget_author_id &lt;- function(name){\n  # add some warnings\n  if(typeof(name)!=\"character\"){stop(\"Please enter a character vector\")}\n  \n  url &lt;- paste0(\"https://scholar.google.com/citations?view_op=search_authors&mauthors=\", paste(gsub(\" \", \"+\", trimws(name)), sep=\"+\"))\n  \n  id &lt;- rvest::read_html(url) %&gt;%\n    rvest::html_elements(\".gs_ai_name a\") %&gt;%\n    rvest::html_attr(\"href\") %&gt;%\n    stringr::str_extract(\"user=([^&]+)\")%&gt;% #extract id using some regex \n    str_replace(\"user=\", \"\") %&gt;% #clean the string\n    first()\n  \n  #yet another warning\n  if(rlang::is_empty(id) | is.na(id)){\n    stop(\"No author was found\")\n  }else{\n    return(id)\n  }\n}\n\n\nNow we define a function to scrap 20 of the papers from this author.\n\n\nShow the code\nget_author_papers &lt;- function(name, by=c(\"recent\", \"citations\")){\n  url &lt;- paste0(\"https://scholar.google.com/citations?hl=fr&user=\", get_author_id(name), \"&view_op=list_works\")\n  \n  #add an option for selecting paper based on recent or number of citation\n  if(by==\"recent\"){url &lt;- paste0(url, \"&sortby=pubdate\")}\n  \n  title &lt;- url %&gt;% \n    rvest::read_html() %&gt;%\n    rvest::html_elements(\".gsc_a_at\")%&gt;%\n    rvest::html_text() %&gt;%\n    tibble::as_tibble() %&gt;%\n    rename(title=value)\n  \n  year &lt;- url %&gt;% \n    rvest::read_html() %&gt;%\n    rvest::html_elements(\".gsc_a_hc\")%&gt;%\n    rvest::html_text() %&gt;%\n    tibble::as_tibble() %&gt;%\n    rename(year=value)\n  \n  #We need to do a bit of cleaning to remove the author's name from coauthors.\n  #the strategy is the following: find the name of the author in the string and remove anything between the two commas where the name is.\n  \n  surname &lt;- str_split(name, \" \") %&gt;% unlist() \n  surname &lt;- surname[[length(surname)]]\n  coauth &lt;- \n    url %&gt;% \n    rvest::read_html() %&gt;%\n    rvest::html_elements(\".gsc_a_at+ .gs_gray\")%&gt;%\n    rvest::html_text() %&gt;% \n    #remove the part of the string containing the last name of the author \n    #and everything before and after in between commas\n    str_replace(paste0(\"(?i)(?&lt;=,|^)[^,]*\", surname, \"[^,]*(?=,|$)\"), \"\") %&gt;%\n    #remove leading or trailing commas\n    str_replace(\"^[,\\\\s]+|[,\\\\s]+$\", \"\")%&gt;%\n    #replace two commas following each other by only one\n    str_replace(\",,\", \",\")%&gt;% \n    tibble::as_tibble() %&gt;%\n    rename(coauth=value) \n  \n  return(tibble(title, year, coauth))\n}\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nA small drawback of my method to get rid of the author’s name among coauthors is that if two persons with the same last name publish a paper together (eg: a couple), then one of them will be removed (first occurrence of the last name). Although this can be a problem, it is unlikely to happen very often, and I have no other solution to offer as I cannot perform matching on the whole author’s name, as first and middle names are usually abbreviated in different ways.\n\n\n\nLet’s display those information for Daron Acemoglu.\n\n\nShow the code\nget_author_papers(\"Daron Acemoglu\", by=\"recent\") %&gt;%\n  reactable(\n    searchable = TRUE, \n    striped = TRUE, \n    resizable = TRUE\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s next\n\n\n\nHopefully one day I’ll learn how to interact with JS components to load more than 20 papers. For this to happen, I need Nelson’s blog to be completed. The current status of the workshop is: 🛠️ Future section 🛠\n\n\n\n\n2. Scrap Wikipedia data\nLoad dependencies\nI’ve faced so many problems using rvest to try to scrap data from Wikipedia. For most pages, using the Selector gadget doesn’t work, I’ve relied on the developer tools in Chrome and using xpath instead of css selectors…\nI start by retrieving data on CO2 emissions on Wikipedia\n\n\nShow the code\nco2_change &lt;- rvest::read_html(\"https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions\") %&gt;%\n  html_node(xpath='/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table') %&gt;%\n  html_table() %&gt;%\n  select(1,9) %&gt;%\n  rename(country=1, co2=2) %&gt;%\n  mutate(co2=as.numeric(gsub(\",|%\", \"\", co2))/100) %&gt;%\n  filter(!row_number() %in% c(1,210:213)) %&gt;%\n  mutate(co2_index=log(co2))\n\nworld_sf &lt;- rnaturalearth::ne_download(returnclass = \"sf\")\n\n\nReading layer `ne_110m_admin_0_countries' from data source \n  `C:\\Users\\verri\\AppData\\Local\\Temp\\RtmpEnXUbo\\ne_110m_admin_0_countries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 177 features and 168 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n\n\nWe now create a map out of it.\n\n\nShow the code\nco2_change %&gt;%\n  dplyr::mutate(iso3c = countrycode::countrycode(country, \"country.name\", \"iso3c\")) |&gt;\n  dplyr::right_join(world_sf |&gt;\n                      dplyr::select(ISO_A3_EH, geometry),\n                    by = c(\"iso3c\" = \"ISO_A3_EH\")) |&gt;\n  ggplot() +\n  geom_sf(\n    aes(fill = co2_index,\n        geometry = geometry),\n    alpha = 0.5) +\n  ggplot2::scale_fill_gradientn(\n    colours=c(\"darkgreen\", \"lightgreen\", \"yellow\", \"darkred\"),\n    values = scales::rescale(c(-2,0,0.001, 2.5)), \n    na.value = \"white\",\n    limits = c(-2, 2.5),\n    oob = scales::oob_squish, \n    breaks=c(-2,0,2.5),\n    labels=c(\"-2 (decrease)\", \"0 (equal)\", \"2.5 (increase)\"),\n    name=\"Evolution (in log)\"\n  )+\n  labs(\n    title = \"CO2 Emissions evolution since 1990\",\n    caption = \"Source: Crippa et al (2023). GHG emissions of all world countries – 2023. Luxembourg: Publications Office of the European Union. Retrieved from Wikipedia. \"\n  ) +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/snippets/econometrics.html",
    "href": "projects/snippets/econometrics.html",
    "title": "Econometrics",
    "section": "",
    "text": "R\n\n\n# set y as factor\ny &lt;- factor(y, levels = c(0, 1), labels = c(\"zero\", \"one\"))\n\n#logit\nlogit &lt;- glm(y ~ d + X, data = mydata, family = binomial(link = \"logit\"))\nsummary(logit)\n\n#probit\nprobit &lt;- glm(y ~ d + X, data = mydata, family = binomial(link = \"probit\"))\nsummary(probit)\nadd margins and how to compute marginal effects here\n\n\n\n\n\n\n\nRPythonStata\n\n\nI use base R, fixest::feols() and lfe:felm().\n# Use base::mtcars data\nols &lt;- lm(mpg ~ wt + ., data = mtcars)\nsummary(ols)\nAdding FE\nlibrary(stargazer)\nlibrary(lfe)\nlibrary(fixest)\n\ndata(mtcars)\nmtcars &lt;- mtcars %&gt;% mutate(year=rep(seq(2000,2003),8)) #add years to the data\n\nreg1 &lt;- lm(mpg ~ wt + as.factor(year), mtcars)\nreg2 &lt;- lfe::felm(mpg ~ wt|year|0|0, mtcars)\nreg3 &lt;- fixest::feols(mpg ~ wt | year, data = mtcars)\n\n#compare; all those methods are equivalent\nstargazer::stargazer(reg1, reg2, type = 'text', keep = \"wt\")\nsummary(reg3, se = \"standard\") \n#Note: feols is not supported by stargazer :/ \nCorrecting for heteroskedasticity (robust SE)\nlibrary(lmtest)\nlibrary(sandwich)\nlibrary(lfe)\nlibrary(fixest)\n\n\n#load data\ndata(mtcars)\n\n# All those methods are equivalent\nreg1 &lt;- lm(mpg ~ wt, mtcars)\nreg1 &lt;- coeftest(reg1, vcov. = vcovHC(reg1, type = \"HC1\"))\nsummary(reg1)\n\nreg2 &lt;- lfe::felm(mpg ~ wt|0|0|0, mtcars)\nsummary(reg2, robust=TRUE)\n\nreg3 &lt;- fixest::feols(mpg ~ wt, data=mtcars, vcov = 'HC1')\nsummary(reg3)\n\n\n# options for type={const, HC0, ..., HC5}\n# const for constant variance (usual estimates)\n# HC0 is White's estimator\n# HC3 is the default as recommended per Long & Ervin (2000)\n# more recent research suggest using HC4m (small sample) or HC5\nClustering SE\nlibrary(lmtest)\nlibrary(sandwich)\nlibrary(lfe)\nlibrary(fixest)\n\ndata(mtcars)\nmtcars &lt;- mtcars %&gt;% mutate(year=rep(seq(2000,2003),8)) #add years to the data\n\n#cluster SE by year; all those methods are equivalent\nreg1 &lt;- lm(mpg ~ wt, mtcars)\nreg1 &lt;- coeftest(reg1, vcov. = vcovCL(reg1, cluster=~year))\nprint(reg1)\n\nreg2 &lt;- lfe::felm(mpg ~ wt|0|0|year, mtcars)\nsummary(reg2)\n\nreg3 &lt;- fixest::feols(mpg ~ wt , data = mtcars, cluster = \"year\")\nsummary(reg3)\n\n\n\n# USING STATS MODEL\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\n\nmtcars = sm.datasets.get_rdataset('mtcars')\nmtcars = pd.DataFrame(mtcars.data)\n\n#first specify formula\nX = [col for col in mtcars.columns if col not in ['mpg', 'wt']]\nformula = 'mpg ~ wt + ' + ' + '.join(X)\nmodel = sm.formula.ols(formula, mtcars)\n\n#fit the model\nmodel = model.fit()\nprint(model.summary())\n\n#when specifying the formula and then fitting, there is no need to add a constant, however when using sm.OLS, you do. \nX = mtcars.drop(columns=[\"mpg\"])\nX = sm.add_constant(X)  # Adds intercept term (column of 1s)\ny = mtcars['mpg']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\nCorrecting for heteroskedasticity (robust se)\n\nrobust_results = model.get_robustcov_results(cov_type='HC3')\nprint(robust_results.summary())\n\nClustered se (account for correlation within groups)\n\nrobust_results = model.get_robustcov_results(cov_type='cluster', groups=mtcars['gear'])\nprint(robust_results.summary())\n\n\n\nreg y x1 x2\nreg y x1 x2, robust\nreg y x1 x2, vce(cluster groupvar)"
  },
  {
    "objectID": "projects/snippets/econometrics.html#binary-outcome",
    "href": "projects/snippets/econometrics.html#binary-outcome",
    "title": "Econometrics",
    "section": "",
    "text": "R\n\n\n# set y as factor\ny &lt;- factor(y, levels = c(0, 1), labels = c(\"zero\", \"one\"))\n\n#logit\nlogit &lt;- glm(y ~ d + X, data = mydata, family = binomial(link = \"logit\"))\nsummary(logit)\n\n#probit\nprobit &lt;- glm(y ~ d + X, data = mydata, family = binomial(link = \"probit\"))\nsummary(probit)\nadd margins and how to compute marginal effects here"
  },
  {
    "objectID": "projects/snippets/econometrics.html#continuous-outcome",
    "href": "projects/snippets/econometrics.html#continuous-outcome",
    "title": "Econometrics",
    "section": "",
    "text": "RPythonStata\n\n\nI use base R, fixest::feols() and lfe:felm().\n# Use base::mtcars data\nols &lt;- lm(mpg ~ wt + ., data = mtcars)\nsummary(ols)\nAdding FE\nlibrary(stargazer)\nlibrary(lfe)\nlibrary(fixest)\n\ndata(mtcars)\nmtcars &lt;- mtcars %&gt;% mutate(year=rep(seq(2000,2003),8)) #add years to the data\n\nreg1 &lt;- lm(mpg ~ wt + as.factor(year), mtcars)\nreg2 &lt;- lfe::felm(mpg ~ wt|year|0|0, mtcars)\nreg3 &lt;- fixest::feols(mpg ~ wt | year, data = mtcars)\n\n#compare; all those methods are equivalent\nstargazer::stargazer(reg1, reg2, type = 'text', keep = \"wt\")\nsummary(reg3, se = \"standard\") \n#Note: feols is not supported by stargazer :/ \nCorrecting for heteroskedasticity (robust SE)\nlibrary(lmtest)\nlibrary(sandwich)\nlibrary(lfe)\nlibrary(fixest)\n\n\n#load data\ndata(mtcars)\n\n# All those methods are equivalent\nreg1 &lt;- lm(mpg ~ wt, mtcars)\nreg1 &lt;- coeftest(reg1, vcov. = vcovHC(reg1, type = \"HC1\"))\nsummary(reg1)\n\nreg2 &lt;- lfe::felm(mpg ~ wt|0|0|0, mtcars)\nsummary(reg2, robust=TRUE)\n\nreg3 &lt;- fixest::feols(mpg ~ wt, data=mtcars, vcov = 'HC1')\nsummary(reg3)\n\n\n# options for type={const, HC0, ..., HC5}\n# const for constant variance (usual estimates)\n# HC0 is White's estimator\n# HC3 is the default as recommended per Long & Ervin (2000)\n# more recent research suggest using HC4m (small sample) or HC5\nClustering SE\nlibrary(lmtest)\nlibrary(sandwich)\nlibrary(lfe)\nlibrary(fixest)\n\ndata(mtcars)\nmtcars &lt;- mtcars %&gt;% mutate(year=rep(seq(2000,2003),8)) #add years to the data\n\n#cluster SE by year; all those methods are equivalent\nreg1 &lt;- lm(mpg ~ wt, mtcars)\nreg1 &lt;- coeftest(reg1, vcov. = vcovCL(reg1, cluster=~year))\nprint(reg1)\n\nreg2 &lt;- lfe::felm(mpg ~ wt|0|0|year, mtcars)\nsummary(reg2)\n\nreg3 &lt;- fixest::feols(mpg ~ wt , data = mtcars, cluster = \"year\")\nsummary(reg3)\n\n\n\n# USING STATS MODEL\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\n\nmtcars = sm.datasets.get_rdataset('mtcars')\nmtcars = pd.DataFrame(mtcars.data)\n\n#first specify formula\nX = [col for col in mtcars.columns if col not in ['mpg', 'wt']]\nformula = 'mpg ~ wt + ' + ' + '.join(X)\nmodel = sm.formula.ols(formula, mtcars)\n\n#fit the model\nmodel = model.fit()\nprint(model.summary())\n\n#when specifying the formula and then fitting, there is no need to add a constant, however when using sm.OLS, you do. \nX = mtcars.drop(columns=[\"mpg\"])\nX = sm.add_constant(X)  # Adds intercept term (column of 1s)\ny = mtcars['mpg']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\nCorrecting for heteroskedasticity (robust se)\n\nrobust_results = model.get_robustcov_results(cov_type='HC3')\nprint(robust_results.summary())\n\nClustered se (account for correlation within groups)\n\nrobust_results = model.get_robustcov_results(cov_type='cluster', groups=mtcars['gear'])\nprint(robust_results.summary())\n\n\n\nreg y x1 x2\nreg y x1 x2, robust\nreg y x1 x2, vce(cluster groupvar)"
  },
  {
    "objectID": "projects/snippets/text_processing.html",
    "href": "projects/snippets/text_processing.html",
    "title": "Text Processing",
    "section": "",
    "text": "Some resources:\n\nJulia Silge & David Robinson, Text Mining with R: A Tidy ApproachJulia Silge and David Robinson (2025) link\nJulia Silge & Emil Hvitfeldt, Supervised Machine Learning for Text Analysis in R (2022) link\n\n\nLoading PDF files\nUse Total sustainability report 2019\n\nRPython\n\n\nlibrary(pdftools)\n\nfile = \"total_rapport_climat_2019_en.pdf\"\n\npages &lt;- pdf_text(file)\npage6 &lt;- pages[6]\n\n\n\nIn Python, there are 2 mains libraries for reading PDF files: pypdf, which is the merging of pyPdf and PyPDF2 [history] and PyMuPDF.\nfile = \"total_rapport_climat_2019_en.pdf\"\n\n#using pypdf\nimport pypdf\n\nreader = pypdf.PdfReader(file)\n\n#extract the text on the 6th page\npage6 = reader.pages[5].extract_text(0)\n\n#store each page in a list\npages = [reader.pages[page_num].extract_text() for page_num in range(len(reader.pages))]\n\n\n#using pymupdf\nimport pymupdf\ndoc = pymupdf.open(file)\n\n#extract the text on the 6th page\npage6 = doc.load_page(5).get_text()\n\n#store each page in a list\npages = [doc.load_page(page_num).get_text(\"text\") for page_num in range(doc.page_count)]\n\n\n\n#merge list element into one 1 element\nraw_text = []\nraw_texts.append(pages)  \n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/snippets.html",
    "href": "projects/snippets.html",
    "title": "Code snippets",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n\n\n\n\n\nTitle\n\n\n\n\n\n\nData wrangling\n\n\n\n\nEconometrics\n\n\n\n\nStatistics\n\n\n\n\nText Processing\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]